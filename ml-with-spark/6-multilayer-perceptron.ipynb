{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417315e1",
   "metadata": {},
   "source": [
    "# Chapter 6\n",
    "\n",
    "## Multilayer perceptron regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"Intro\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"Make\", StringType(), True),\n",
    "    StructField(\"Model\", StringType(), True),\n",
    "    StructField(\"Vehicle Class\", StringType(), True),\n",
    "    StructField(\"Cylinders\", DoubleType(), True),\n",
    "    StructField(\"Transmission\", StringType(), True),\n",
    "    StructField(\"Fuel Type\", StringType(), True),\n",
    "    StructField(\"Fuel Consumption City (L/100 km)\", DoubleType(), True),\n",
    "    StructField(\"Fuel Consumption Hwy (L/100 km)\", DoubleType(), True),\n",
    "    StructField(\"Fuel Consumption Comb (L/100 km)\", DoubleType(), True),\n",
    "    StructField(\"Fuel Consumption Comb (mpg)\", DoubleType(), True),\n",
    "    StructField(\"CO2\", DoubleType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "co2_data = spark.read.format(\"csv\")\\\n",
    "    .schema(custom_schema) \\\n",
    "    .option(\"header\", True) \\\n",
    "    .load(\"../datasets/CO2_Emissions_Canada.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_only_continues_values = {'Fuel Consumption City (L/100 km)':0}\n",
    "#                               \"Fuel Consumption Hwy (L/100 km)\",\n",
    "#         \"Fuel Consumption Comb (L/100 km)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_data = co2_data.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-shuttle",
   "metadata": {},
   "source": [
    "# Prep the data for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-reaction",
   "metadata": {},
   "source": [
    "turn the feature columns into one indexed column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cols = [\"Make\", \"Model\", \"Vehicle Class\",\"Cylinders\",\"Transmission\",\"Fuel Type\",\n",
    "        \"Fuel Consumption City (L/100 km)\", \"Fuel Consumption Hwy (L/100 km)\",\n",
    "        \"Fuel Consumption Comb (L/100 km)\",\"Fuel Consumption Comb (mpg)\"]\n",
    "\n",
    "cols_only_continues = [\"Fuel Consumption City (L/100 km)\", \"Fuel Consumption Hwy (L/100 km)\",\n",
    "        \"Fuel Consumption Comb (L/100 km)\"]\n",
    "\n",
    "hasher = FeatureHasher(outputCol=\"hashed_features\", inputCols=cols_only_continues)\n",
    "co2_data = hasher.transform(co2_data)\n",
    "                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_data.select(\"hashed_features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_data.select(\"hashed_features\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_data.select(\"hashed_features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-prevention",
   "metadata": {},
   "source": [
    "# time for selecting the most meaninful features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import UnivariateFeatureSelector\n",
    "\n",
    "selector = UnivariateFeatureSelector(outputCol=\"selectedFeatures\", featuresCol=\"hashed_features\", labelCol=\"CO2\")\n",
    "\n",
    "selector.setFeatureType(\"continuous\")\n",
    "selector.setLabelType(\"continuous\")\n",
    "\n",
    "model = selector.fit(co2_data)\n",
    "output = model.transform(co2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.select(\"selectedFeatures\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-advancement",
   "metadata": {},
   "source": [
    "Count the number of available classes as it is needed as part of the last layer for the multilayer perceptron classifier.\n",
    "\n",
    "Classifier trainer based on the Multilayer Perceptron. \n",
    "Each layer has sigmoid activation function, output layer has softmax. \n",
    "\n",
    "Number of inputs has to be equal to the size of feature vectors. \n",
    "\n",
    "Number of outputs has to be equal to the total number of labels.\n",
    "\n",
    "\n",
    "Specify the layers for the neural network as follows: \n",
    "\n",
    "input layer => size 50 (features), two intermediate layers (i.e. hidden layer) of size 20 and 8 and output => size 70 as the largest value of CO2 is 69 and lables array is searched by index (classes).      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(layers=[50,20, 8, 70], seed=123, featuresCol=\"selectedFeatures\", labelCol=\"CO2\")\n",
    "mlp.setMaxIter(100)\n",
    "\n",
    "\n",
    "model = mlp.fit(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-library",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
