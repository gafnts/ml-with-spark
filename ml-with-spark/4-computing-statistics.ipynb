{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mobile-switzerland",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "\n",
    "## Computing statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-virus",
   "metadata": {},
   "source": [
    "\n",
    "Use Machine Learning Methods to Correctly Classify Animals Based Upon Attributes.\n",
    "Dataset by Kaggle. More information can be found [here](https://www.kaggle.com/uciml/zoo-animal-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "qualified-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Intro\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-korea",
   "metadata": {},
   "source": [
    "# Provide custome schema for the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "orange-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "\n",
    "custom_schema = StructType(\n",
    "    [\n",
    "        StructField(\"animal_name\", StringType(), True),\n",
    "        StructField(\"hair\", DoubleType(), True),\n",
    "        StructField(\"feathers\", DoubleType(), True),\n",
    "        StructField(\"eggs\", DoubleType(), True),\n",
    "        StructField(\"milk\", DoubleType(), True),\n",
    "        StructField(\"airborne\", DoubleType(), True),\n",
    "        StructField(\"aquatic\", DoubleType(), True),\n",
    "        StructField(\"predator\", DoubleType(), True),\n",
    "        StructField(\"toothed\", DoubleType(), True),\n",
    "        StructField(\"backbone\", DoubleType(), True),\n",
    "        StructField(\"breathes\", DoubleType(), True),\n",
    "        StructField(\"legs\", DoubleType(), True),\n",
    "        StructField(\"tail\", DoubleType(), True),\n",
    "        StructField(\"domestic\", DoubleType(), True),\n",
    "        StructField(\"catsize\", DoubleType(), True),\n",
    "        StructField(\"class_type\", StringType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advisory-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path().cwd() / \"static\" / \"zoo.csv\"\n",
    "\n",
    "zoo_data = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .schema(custom_schema)\n",
    "    .option(\"header\", True)\n",
    "    .load(str(data_path))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "varied-element",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/21 19:45:39 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 18, schema size: 16\n",
      "CSV file: file:///Users/gafnts/Documents/Github/ml-with-spark/ml-with-spark/static/zoo.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(animal_name='aardvark', hair=1.0, feathers=0.0, eggs=0.0, milk=1.0, airborne=0.0, aquatic=0.0, predator=1.0, toothed=1.0, backbone=1.0, breathes=1.0, legs=0.0, tail=0.0, domestic=4.0, catsize=0.0, class_type='0')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zoo_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "under-clinic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- animal_name: string (nullable = true)\n",
      " |-- hair: double (nullable = true)\n",
      " |-- feathers: double (nullable = true)\n",
      " |-- eggs: double (nullable = true)\n",
      " |-- milk: double (nullable = true)\n",
      " |-- airborne: double (nullable = true)\n",
      " |-- aquatic: double (nullable = true)\n",
      " |-- predator: double (nullable = true)\n",
      " |-- toothed: double (nullable = true)\n",
      " |-- backbone: double (nullable = true)\n",
      " |-- breathes: double (nullable = true)\n",
      " |-- legs: double (nullable = true)\n",
      " |-- tail: double (nullable = true)\n",
      " |-- domestic: double (nullable = true)\n",
      " |-- catsize: double (nullable = true)\n",
      " |-- class_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zoo_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-closure",
   "metadata": {},
   "source": [
    "# Calculate statistics\n",
    "for this, we will use the Summarizer functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unexpected-hudson",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/21 19:45:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: hair, feathers, eggs, milk, airborne, aquatic, predator, toothed, backbone, breathes, venomous, fins, legs, tail, domestic\n",
      " Schema: hair, feathers, eggs, milk, airborne, aquatic, predator, toothed, backbone, breathes, legs, tail, domestic, catsize, class_type\n",
      "Expected: legs but found: venomous\n",
      "CSV file: file:///Users/gafnts/Documents/Github/ml-with-spark/ml-with-spark/static/zoo.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(hair=1.0, feathers=0.0, eggs=0.0, milk=1.0, airborne=0.0, aquatic=0.0, predator=1.0, toothed=1.0, backbone=1.0, breathes=1.0, legs=0.0, tail=0.0, domestic=4.0, catsize=0.0, class_type='0'),\n",
       " Row(hair=1.0, feathers=0.0, eggs=0.0, milk=1.0, airborne=0.0, aquatic=0.0, predator=0.0, toothed=1.0, backbone=1.0, breathes=1.0, legs=0.0, tail=0.0, domestic=4.0, catsize=1.0, class_type='0'),\n",
       " Row(hair=0.0, feathers=0.0, eggs=1.0, milk=0.0, airborne=0.0, aquatic=1.0, predator=1.0, toothed=1.0, backbone=1.0, breathes=0.0, legs=0.0, tail=1.0, domestic=0.0, catsize=1.0, class_type='0'),\n",
       " Row(hair=1.0, feathers=0.0, eggs=0.0, milk=1.0, airborne=0.0, aquatic=0.0, predator=1.0, toothed=1.0, backbone=1.0, breathes=1.0, legs=0.0, tail=0.0, domestic=4.0, catsize=0.0, class_type='0'),\n",
       " Row(hair=1.0, feathers=0.0, eggs=0.0, milk=1.0, airborne=0.0, aquatic=0.0, predator=1.0, toothed=1.0, backbone=1.0, breathes=1.0, legs=0.0, tail=0.0, domestic=4.0, catsize=1.0, class_type='0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zoo_data_for_statistics = zoo_data.drop(\"animal_name\", \"lass_type\")\n",
    "\n",
    "zoo_data_for_statistics.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-tulsa",
   "metadata": {},
   "source": [
    "## Turn the columns into a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-carrier",
   "metadata": {},
   "source": [
    "Notice that for simplifying the example, we are going to examin the following columns:\n",
    "\n",
    "* feathers\n",
    "* milk\n",
    "* fins\n",
    "* domestic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "expanded-brake",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "[FIELD_NOT_FOUND] No such struct field `fins` in `hair`, `feathers`, `eggs`, `milk`, `airborne`, `aquatic`, `predator`, `toothed`, `backbone`, `breathes`, `legs`, `tail`, `domestic`, `catsize`, `class_type`. SQLSTATE: 42704",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m vecAssembler = VectorAssembler(outputCol=\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m vecAssembler.setInputCols([\u001b[33m\"\u001b[39m\u001b[33mfeathers\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmilk\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfins\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdomestic\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m vector_df = \u001b[43mvecAssembler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzoo_data_for_statistics\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/ml-with-spark/.venv/lib/python3.12/site-packages/pyspark/ml/base.py:260\u001b[39m, in \u001b[36mTransformer.transform\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._transform(dataset)\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/ml-with-spark/.venv/lib/python3.12/site-packages/pyspark/ml/util.py:212\u001b[39m, in \u001b[36mtry_remote_transform_relation.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    210\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/ml-with-spark/.venv/lib/python3.12/site-packages/pyspark/ml/wrapper.py:429\u001b[39m, in \u001b[36mJavaTransformer._transform\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/ml-with-spark/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/ml-with-spark/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: [FIELD_NOT_FOUND] No such struct field `fins` in `hair`, `feathers`, `eggs`, `milk`, `airborne`, `aquatic`, `predator`, `toothed`, `backbone`, `breathes`, `legs`, `tail`, `domestic`, `catsize`, `class_type`. SQLSTATE: 42704"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(outputCol=\"features\")\n",
    "vecAssembler.setInputCols([\"feathers\", \"milk\", \"fins\", \"domestic\"])\n",
    "\n",
    "vector_df = vecAssembler.transform(zoo_data_for_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1d7962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hair=1.0, feathers=0.0, eggs=0.0, milk=1.0, airborne=0.0, aquatic=0.0, predator=1.0, toothed=1.0, backbone=1.0, breathes=1.0, venomous=0.0, fins=0.0, legs=4.0, tail=0.0, domestic=0.0, catsize=1.0, class_type='1', features=SparseVector(4, {1: 1.0})),\n",
       " Row(hair=1.0, feathers=0.0, eggs=0.0, milk=1.0, airborne=0.0, aquatic=0.0, predator=0.0, toothed=1.0, backbone=1.0, breathes=1.0, venomous=0.0, fins=0.0, legs=4.0, tail=1.0, domestic=0.0, catsize=1.0, class_type='1', features=SparseVector(4, {1: 1.0})),\n",
       " Row(hair=0.0, feathers=0.0, eggs=1.0, milk=0.0, airborne=0.0, aquatic=1.0, predator=1.0, toothed=1.0, backbone=1.0, breathes=0.0, venomous=0.0, fins=1.0, legs=0.0, tail=1.0, domestic=0.0, catsize=0.0, class_type='4', features=SparseVector(4, {2: 1.0})),\n",
       " Row(hair=1.0, feathers=0.0, eggs=0.0, milk=1.0, airborne=0.0, aquatic=0.0, predator=1.0, toothed=1.0, backbone=1.0, breathes=1.0, venomous=0.0, fins=0.0, legs=4.0, tail=0.0, domestic=0.0, catsize=1.0, class_type='1', features=SparseVector(4, {1: 1.0})),\n",
       " Row(hair=1.0, feathers=0.0, eggs=0.0, milk=1.0, airborne=0.0, aquatic=0.0, predator=1.0, toothed=1.0, backbone=1.0, breathes=1.0, venomous=0.0, fins=0.0, legs=4.0, tail=1.0, domestic=0.0, catsize=1.0, class_type='1', features=SparseVector(4, {1: 1.0}))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-reliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hair: double (nullable = true)\n",
      " |-- feathers: double (nullable = true)\n",
      " |-- eggs: double (nullable = true)\n",
      " |-- milk: double (nullable = true)\n",
      " |-- airborne: double (nullable = true)\n",
      " |-- aquatic: double (nullable = true)\n",
      " |-- predator: double (nullable = true)\n",
      " |-- toothed: double (nullable = true)\n",
      " |-- backbone: double (nullable = true)\n",
      " |-- breathes: double (nullable = true)\n",
      " |-- venomous: double (nullable = true)\n",
      " |-- fins: double (nullable = true)\n",
      " |-- legs: double (nullable = true)\n",
      " |-- tail: double (nullable = true)\n",
      " |-- domestic: double (nullable = true)\n",
      " |-- catsize: double (nullable = true)\n",
      " |-- class_type: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-proceeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|aggregate_metrics(features, 1.0)                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{[0.19801980198019803,0.40594059405940597,0.16831683168316833,0.12871287128712872], [0.1603960396039604,0.24356435643564356,0.1413861386138614,0.11326732673267326], [20.0,41.0,17.0,13.0], [4.47213595499958,6.4031242374328485,4.123105625617661,3.605551275463989], [0.4004947435409863,0.4935223970962651,0.37601348195757744,0.33655211592363116], [20.0,41.0,17.0,13.0], [20.0,41.0,17.0,13.0], [1.0,1.0,1.0,1.0], [0.0,0.0,0.0,0.0]}|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "summarizer = Summarizer.metrics(\n",
    "    \"mean\", \"variance\", \"normL1\", \"normL2\", \"std\", \"sum\", \"numNonZeros\", \"max\", \"min\"\n",
    ")\n",
    "\n",
    "statistics_df = vector_df.select(summarizer.summary(vector_df.features))\n",
    "\n",
    "statistics_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-listing",
   "metadata": {},
   "source": [
    "Notice that statistics dataframe has only one column named aggregate_metrics, where aggregate_metrics coluumns has more columns, where each one of them is a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-corner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aggregate_metrics(features, 1.0): struct (nullable = false)\n",
      " |    |-- mean: vector (nullable = false)\n",
      " |    |-- variance: vector (nullable = false)\n",
      " |    |-- normL1: vector (nullable = false)\n",
      " |    |-- normL2: vector (nullable = false)\n",
      " |    |-- std: vector (nullable = false)\n",
      " |    |-- sum: vector (nullable = false)\n",
      " |    |-- numNonZeros: vector (nullable = false)\n",
      " |    |-- max: vector (nullable = false)\n",
      " |    |-- min: vector (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statistics_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-investment",
   "metadata": {},
   "source": [
    "For enabling easier access to the data, we use explode functionality that flattens one hirarchy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-basement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------+\n",
      "|std(features)                                                                  |\n",
      "+-------------------------------------------------------------------------------+\n",
      "|[0.4004947435409863,0.4935223970962651,0.37601348195757744,0.33655211592363116]|\n",
      "+-------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_df.select(Summarizer.std(vector_df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-insulin",
   "metadata": {},
   "source": [
    "From [wikipedia](https://en.wikipedia.org/wiki/Standard_deviation) std - Standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. \n",
    "\n",
    "Looking at the vector results, the distance from the among each individual feature is lower than 0.5\n",
    "Our features: \"feathers\",\"milk\",\"fins\",\"domestic\"\n",
    "\n",
    "The reson for it, mainly is, the data should be represented in boolean since each feature is a yes/no fearure.\n",
    "Feathers =1 , means that this animal has feathers and so on.\n",
    "\n",
    "Now that we know this, let's take a look at count, which will tell us how many animals in the database has feathers, milk, fins or domestic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-wholesale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|sum(features)        |\n",
      "+---------------------+\n",
      "|[20.0,41.0,17.0,13.0]|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_df.select(Summarizer.sum(vector_df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-philadelphia",
   "metadata": {},
   "source": [
    "`sum` provides us with a more relatable information that we can use to understand the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-purse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------+\n",
      "|variance(features)                                                             |\n",
      "+-------------------------------------------------------------------------------+\n",
      "|[0.1603960396039604,0.24356435643564356,0.1413861386138614,0.11326732673267326]|\n",
      "+-------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_df.select(Summarizer.variance(vector_df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-fellow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|count(features)|\n",
      "+---------------+\n",
      "|101            |\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_df.select(Summarizer.count(vector_df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-insider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|numNonZeros(features)|\n",
      "+---------------------+\n",
      "|[20.0,41.0,17.0,13.0]|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_df.select(Summarizer.numNonZeros(vector_df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-thread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|max(features)    |\n",
      "+-----------------+\n",
      "|[1.0,1.0,1.0,1.0]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_df.select(Summarizer.max(vector_df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-empty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|normL1(features)     |\n",
      "+---------------------+\n",
      "|[20.0,41.0,17.0,13.0]|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_df.select(Summarizer.normL1(vector_df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-midwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|normL2(features)                                                         |\n",
      "+-------------------------------------------------------------------------+\n",
      "|[4.47213595499958,6.4031242374328485,4.123105625617661,3.605551275463989]|\n",
      "+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_df.select(Summarizer.normL2(vector_df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-hormone",
   "metadata": {},
   "source": [
    "# Testing features correlations\n",
    "As part of understanding each featres statistics on its own, let's understand the correlation between the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-living",
   "metadata": {},
   "source": [
    "### Notice\n",
    "This functionality also requires a vector, we will use the one from the earlier computation - `vector_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-paradise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[ 1.        , -0.41076061, -0.22354106,  0.03158624],\n",
      "             [-0.41076061,  1.        , -0.15632771,  0.16392762],\n",
      "             [-0.22354106, -0.15632771,  1.        , -0.09388671],\n",
      "             [ 0.03158624,  0.16392762, -0.09388671,  1.        ]])\n",
      "\n",
      "Spearman correlation matrix:\n",
      "DenseMatrix([[ 1.        , -0.41076061, -0.22354106,  0.03158624],\n",
      "             [-0.41076061,  1.        , -0.15632771,  0.16392762],\n",
      "             [-0.22354106, -0.15632771,  1.        , -0.09388671],\n",
      "             [ 0.03158624,  0.16392762, -0.09388671,  1.        ]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "r1 = Correlation.corr(vector_df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]) + \"\\n\")\n",
    "\n",
    "r2 = Correlation.corr(vector_df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-coaching",
   "metadata": {},
   "source": [
    "Breakdown of the correlation metrix is in the book, chapter 3 under statistics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-toronto",
   "metadata": {},
   "source": [
    "## ChiSquareTest\n",
    "\n",
    "Testing the p-value of the columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-administration",
   "metadata": {},
   "source": [
    "This requeires vector as well Hence we will use the prcompute vector from before. \n",
    "\n",
    "Notice that label in this case, has to be of type numberic.\n",
    "To tranform the label into numberic, we will use the StringIndexer transofmer functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"class_type\", outputCol=\"label\")\n",
    "indexed_lable = indexer.fit(vector_df).transform(vector_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-taiwan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hair: double (nullable = true)\n",
      " |-- feathers: double (nullable = true)\n",
      " |-- eggs: double (nullable = true)\n",
      " |-- milk: double (nullable = true)\n",
      " |-- airborne: double (nullable = true)\n",
      " |-- aquatic: double (nullable = true)\n",
      " |-- predator: double (nullable = true)\n",
      " |-- toothed: double (nullable = true)\n",
      " |-- backbone: double (nullable = true)\n",
      " |-- breathes: double (nullable = true)\n",
      " |-- venomous: double (nullable = true)\n",
      " |-- fins: double (nullable = true)\n",
      " |-- legs: double (nullable = true)\n",
      " |-- tail: double (nullable = true)\n",
      " |-- domestic: double (nullable = true)\n",
      " |-- catsize: double (nullable = true)\n",
      " |-- class_type: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_lable.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-daily",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(4, {1: 1.0})),\n",
       " Row(features=SparseVector(4, {1: 1.0})),\n",
       " Row(features=SparseVector(4, {2: 1.0})),\n",
       " Row(features=SparseVector(4, {1: 1.0})),\n",
       " Row(features=SparseVector(4, {1: 1.0}))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_lable.select(\"features\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-burke",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hair=1.0, feathers=0.0, eggs=0.0, milk=1.0, airborne=0.0, aquatic=0.0, predator=1.0, toothed=1.0, backbone=1.0, breathes=1.0, venomous=0.0, fins=0.0, legs=4.0, tail=0.0, domestic=0.0, catsize=1.0, class_type='1', features=SparseVector(4, {1: 1.0}), label=0.0),\n",
       " Row(hair=1.0, feathers=0.0, eggs=0.0, milk=1.0, airborne=0.0, aquatic=0.0, predator=0.0, toothed=1.0, backbone=1.0, breathes=1.0, venomous=0.0, fins=0.0, legs=4.0, tail=1.0, domestic=0.0, catsize=1.0, class_type='1', features=SparseVector(4, {1: 1.0}), label=0.0),\n",
       " Row(hair=0.0, feathers=0.0, eggs=1.0, milk=0.0, airborne=0.0, aquatic=1.0, predator=1.0, toothed=1.0, backbone=1.0, breathes=0.0, venomous=0.0, fins=1.0, legs=0.0, tail=1.0, domestic=0.0, catsize=0.0, class_type='4', features=SparseVector(4, {2: 1.0}), label=2.0),\n",
       " Row(hair=1.0, feathers=0.0, eggs=0.0, milk=1.0, airborne=0.0, aquatic=0.0, predator=1.0, toothed=1.0, backbone=1.0, breathes=1.0, venomous=0.0, fins=0.0, legs=4.0, tail=0.0, domestic=0.0, catsize=1.0, class_type='1', features=SparseVector(4, {1: 1.0}), label=0.0),\n",
       " Row(hair=1.0, feathers=0.0, eggs=0.0, milk=1.0, airborne=0.0, aquatic=0.0, predator=1.0, toothed=1.0, backbone=1.0, breathes=1.0, venomous=0.0, fins=0.0, legs=4.0, tail=1.0, domestic=0.0, catsize=1.0, class_type='1', features=SparseVector(4, {1: 1.0}), label=0.0)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_lable.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-hospital",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(degreesOfFreedom=[6, 6, 6, 6])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "chiSqResult = ChiSquareTest.test(indexed_lable, \"features\", \"label\")\n",
    "chiSqResult.select(\"degreesOfFreedom\").collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-convert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.99999999999999"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chiSqResult = ChiSquareTest.test(indexed_lable, \"features\", \"label\", True)\n",
    "row = chiSqResult.orderBy(\"featureIndex\").collect()\n",
    "row[0].statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-bulletin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(featureIndex=0, pValue=0.0, degreesOfFreedom=6, statistic=100.99999999999999),\n",
       " Row(featureIndex=1, pValue=0.0, degreesOfFreedom=6, statistic=101.0),\n",
       " Row(featureIndex=3, pValue=0.5681588672220808, degreesOfFreedom=6, statistic=4.8118701947677085),\n",
       " Row(featureIndex=2, pValue=3.4638958368304884e-14, degreesOfFreedom=6, statistic=75.21350003415999)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chiSqResult.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6e28e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-with-spark-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
