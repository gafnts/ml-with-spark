{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "modern-klein",
   "metadata": {},
   "source": [
    "# Chapter 6\n",
    "\n",
    "## Gaussian mixture pipeline\n",
    "\n",
    "Use Machine Learning Methods to cluster cars and their CO2 emission. \n",
    "Dataset by Kaggle. More information can be found [here](https://www.kaggle.com/debajyotipodder/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"Make\", StringType(), True),\n",
    "    StructField(\"Model\", StringType(), True),\n",
    "    StructField(\"Vehicle Class\", StringType(), True),\n",
    "    StructField(\"Cylinders\", DoubleType(), True),\n",
    "    StructField(\"Transmission\", StringType(), True),\n",
    "    StructField(\"Fuel Type\", StringType(), True),\n",
    "    StructField(\"Fuel Consumption City (L/100 km)\", DoubleType(), True),\n",
    "    StructField(\"Fuel Consumption Hwy (L/100 km)\", DoubleType(), True),\n",
    "    StructField(\"Fuel Consumption Comb (L/100 km)\", DoubleType(), True),\n",
    "    StructField(\"Fuel Consumption Comb (mpg)\", DoubleType(), True),\n",
    "    StructField(\"CO2\", DoubleType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "co2_data = spark.read.format(\"csv\")\\\n",
    "    .schema(custom_schema) \\\n",
    "    .option(\"header\", True) \\\n",
    "    .load(\"../datasets/CO2_Emissions_Canada.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_data = co2_data.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-sellers",
   "metadata": {},
   "source": [
    "# Build Hasher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-embassy",
   "metadata": {},
   "source": [
    "turn the feature columns into one indexed column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "cols_only_continues = [\"Fuel Consumption City (L/100 km)\", \"Fuel Consumption Hwy (L/100 km)\",\n",
    "        \"Fuel Consumption Comb (L/100 km)\"]\n",
    "\n",
    "hasher = FeatureHasher(outputCol=\"hashed_features\", inputCols=cols_only_continues)\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-roots",
   "metadata": {},
   "source": [
    "# Build Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import UnivariateFeatureSelector\n",
    "\n",
    "selector = UnivariateFeatureSelector(outputCol=\"selectedFeatures\", featuresCol=\"hashed_features\", labelCol=\"CO2\")\n",
    "\n",
    "selector.setFeatureType(\"continuous\")\n",
    "selector.setLabelType(\"continuous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-spain",
   "metadata": {},
   "source": [
    "# Create GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "gm = GaussianMixture(k=42, tol=0.01, seed=10, featuresCol=\"selectedFeatures\", maxIter=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-depth",
   "metadata": {},
   "source": [
    "# Constructing - The Pipeline API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[hasher,selector, gm])\n",
    "# Fit the pipeline to training data.\n",
    "pipeline_model = pipeline.fit(co2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_by_pipeline = pipeline_model.transform(co2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_by_pipeline.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-layout",
   "metadata": {},
   "source": [
    "# Persisting the pipeline to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_with_pip = \"/tmp/pip_model\"\n",
    "pipeline_model.write().overwrite().save(path_model_with_pip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-milton",
   "metadata": {},
   "source": [
    "# Using our model in Stream processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume we have data ingested in stream into our system:\n",
    "data_in_stream = spark \\\n",
    "    .readStream \\\n",
    "    .schema(custom_schema) \\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", True) \\\n",
    "    .load(\"StreamData/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "pipeline_from_disk = PipelineModel.load(path_model_with_pip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, sum\n",
    "\n",
    "transformed_output = pipeline_from_disk.transform(data_in_stream)\\\n",
    "  .agg((sum(when(col('prediction') == 1, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = transformed_output.writeStream.outputMode('complete').queryName(\"spark_streaming_ml\").format('memory').start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import sql\n",
    "\n",
    "output = spark.sql(\"select * from spark_streaming_ml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-addition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
