{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5067dfe",
   "metadata": {},
   "source": [
    "## Working with Text Data - Chapter 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ranking-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"Intro\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8340e25",
   "metadata": {},
   "source": [
    "Define a documentDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stylish-jimmy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.008409444987773896,0.005632373690605164,0.03362197317183018]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [-0.004308507378612245,0.0405282654932567,0.056872056680731475]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [-0.049474555999040604,0.021491939178667964,0.04208827670663595]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(f\"Text: [{', '.join(text)}] => \\nVector: {vector}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-quilt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------------------------------------------------+------+\n",
      "|sentence                                   |words                                                       |tokens|\n",
      "+-------------------------------------------+------------------------------------------------------------+------+\n",
      "|Hi|I|heard|about|Spark                     |[hi|i|heard|about|spark]                                    |1     |\n",
      "|I     wish Java      could use case classes|[i, , , , , wish, java, , , , , , could, use, case, classes]|16    |\n",
      "|Logistic,regression,models,are,neat        |[logistic,regression,models,are,neat]                       |1     |\n",
      "+-------------------------------------------+------------------------------------------------------------+------+\n",
      "\n",
      "+-------------------------------------------+------------------------------------------+------+\n",
      "|sentence                                   |words                                     |tokens|\n",
      "+-------------------------------------------+------------------------------------------+------+\n",
      "|Hi|I|heard|about|Spark                     |[hi, i, heard, about, spark]              |5     |\n",
      "|I     wish Java      could use case classes|[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat        |[logistic, regression, models, are, neat] |5     |\n",
      "+-------------------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame(\n",
    "    [\n",
    "        (0, \"Hi|I|heard|about|Spark\"),\n",
    "        (1, \"I     wish Java      could use case classes\"),\n",
    "        (2, \"Logistic,regression,models,are,neat\")\n",
    "    ], \n",
    "    [\"id\", \"sentence\"]\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\")))\\\n",
    "    .show(truncate=False)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\")))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "multiple-tender",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/04 18:46:33 WARN StopWordsRemover: Default locale set was [en_GT]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filtered            |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sentenceData = spark.createDataFrame(\n",
    "    [\n",
    "        (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "        (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "    ], \n",
    "    [\"id\", \"raw\"]\n",
    ")\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\", locale=\"es\")\n",
    "\n",
    "remover.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-browser",
   "metadata": {},
   "source": [
    "# Code Example -> Tokanizer -> N Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-render",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------+\n",
      "|ngrams                                                           |\n",
      "+-----------------------------------------------------------------+\n",
      "|[hi i, i heard, heard about, about spark]                        |\n",
      "|[i wish,, wish, wish, wish java,, java, java, java could]        |\n",
      "|[logistic regression,, regression, regression, regression models]|\n",
      "+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame(\n",
    "    [\n",
    "        (0, \"Hi I heard about Spark \"),\n",
    "        (1, \"I wish, wish Java, Java could\"),\n",
    "        (2, \"Logistic regression, regression models\")\n",
    "    ], \n",
    "    [\"id\", \"sentence\"]\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "wordDataFrame = tokenizer.transform(sentenceDataFrame)\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-southeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer output with Threshold = 0.500000\n",
      "+---+-------+-----------------+\n",
      "| id|feature|binarized_feature|\n",
      "+---+-------+-----------------+\n",
      "|  0|    5.1|              1.0|\n",
      "|  1|    5.8|              1.0|\n",
      "|  2|    0.2|              0.0|\n",
      "+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "continuousDataFrame = spark.createDataFrame([\n",
    "    (0, 5.1),\n",
    "    (1, 5.8),\n",
    "    (2, 0.2)\n",
    "], [\"id\", \"feature\"])\n",
    "\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
    "\n",
    "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
    "\n",
    "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
    "\n",
    "binarizedDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adequate-statistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|pcaFeatures                                                 |\n",
      "+------------------------------------------------------------+\n",
      "|[1.6485728230883814,-4.0132827005162985,-1.0091435193998504]|\n",
      "|[-4.645104331781533,-1.1167972663619048,-1.0091435193998501]|\n",
      "|[-6.428880535676488,-5.337951427775359,-1.009143519399851]  |\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "\n",
    "result = model.transform(df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "worldwide-miracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------------------------------------------------+\n",
      "|features |polyFeatures                                                                        |\n",
      "+---------+------------------------------------------------------------------------------------+\n",
      "|[2.0,1.0]|[2.0,4.0,8.0,16.0,32.0,1.0,2.0,4.0,8.0,16.0,1.0,2.0,4.0,8.0,1.0,2.0,4.0,1.0,2.0,1.0]|\n",
      "|[0.0,0.0]|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]   |\n",
      "+---------+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (Vectors.dense([2.0, 1.0]),),\n",
    "    (Vectors.dense([0.0, 0.0]),)\n",
    "], [\"features\"])\n",
    "\n",
    "polyExpansion = PolynomialExpansion(degree=5, inputCol=\"features\", outputCol=\"polyFeatures\")\n",
    "polyDF = polyExpansion.transform(df)\n",
    "\n",
    "polyDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71ce0ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                      |word2vec_features                                               |poly_features                                                                                                                                                                                       |\n",
      "+------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[Hi, I, heard, about, Spark]              |[0.008409444987773896,0.005632373690605164,0.03362197317183018] |[0.008409444987773896,7.07187650023955E-5,0.005632373690605164,4.736513670172915E-5,3.172363339062123E-5,0.03362197317183018,2.827421337689157E-4,1.8937151711924895E-4,0.0011304370799672685]      |\n",
      "|[I, wish, Java, could, use, case, classes]|[-0.004308507378612245,0.0405282654932567,0.056872056680731475] |[-0.004308507378612245,1.8563235831556162E-5,0.0405282654932567,-1.7461633092005253E-4,0.0016425403038919017,0.056872056680731475,-2.450336758457854E-4,0.0023049258123042284,0.0032344308310963336]|\n",
      "|[Logistic, regression, models, are, neat] |[-0.049474555999040604,0.021491939178667964,0.04208827670663595]|[-0.049474555999040604,0.0024477316913022046,0.021491939178667964,-0.0010633041484229828,4.61903449659563E-4,0.04208827670663595,-0.0020822988028255766,9.045586831139675E-4,0.0017714230361343545] |\n",
      "+------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec, PolynomialExpansion\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"word2vec_features\")\n",
    "\n",
    "polyExpansion = PolynomialExpansion(degree=2, inputCol=\"word2vec_features\", outputCol=\"poly_features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[word2Vec, polyExpansion])\n",
    "model = pipeline.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "\n",
    "result.select(\"text\", \"word2vec_features\", \"poly_features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "parental-battlefield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n",
      "|featuresDCT                                                     |\n",
      "+----------------------------------------------------------------+\n",
      "|[1.0,-1.1480502970952693,2.0000000000000004,-2.7716385975338604]|\n",
      "|[-1.0,3.378492794482933,-7.000000000000001,2.9301512653149677]  |\n",
      "|[4.0,9.304453421915744,11.000000000000002,1.5579302036357163]   |\n",
      "+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import DCT\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (Vectors.dense([0.0, 1.0, -2.0, 3.0]),),\n",
    "    (Vectors.dense([-1.0, 2.0, 4.0, -7.0]),),\n",
    "    (Vectors.dense([14.0, -2.0, -5.0, 1.0]),)], [\"features\"])\n",
    "\n",
    "dct = DCT(inverse=False, inputCol=\"features\", outputCol=\"featuresDCT\")\n",
    "\n",
    "dctDf = dct.transform(df)\n",
    "\n",
    "dctDf.select(\"featuresDCT\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "drawn-review",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|      scaledFeatures|\n",
      "+--------------------+--------------------+\n",
      "|[1.0,0.1,-8.0,200.0]|[0.25,0.010000000...|\n",
      "|  [2.0,1.0,-4.0,2.0]| [0.5,0.1,-0.5,0.01]|\n",
      "|  [4.0,10.0,8.0,0.0]|   [1.0,1.0,1.0,0.0]|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -8.0,200]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, -4.0,2]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 8.0,0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MaxAbsScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# rescale each feature to range [-1, 1].\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "champion-lyric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketizer output with 4 buckets\n",
      "+--------+----------------+\n",
      "|features|bucketedFeatures|\n",
      "+--------+----------------+\n",
      "|  -999.9|             0.0|\n",
      "|    -0.5|             1.0|\n",
      "|    -0.3|             1.0|\n",
      "|     0.0|             2.0|\n",
      "|     0.2|             2.0|\n",
      "|   999.9|             3.0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
    "\n",
    "data = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)]\n",
    "dataFrame = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n",
    "\n",
    "# Transform original data into its bucket index.\n",
    "bucketedData = bucketizer.transform(dataFrame)\n",
    "\n",
    "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dying-lender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|       vector|transformedVector|\n",
      "+-------------+-----------------+\n",
      "|[1.0,2.0,3.0]|    [0.0,2.0,6.0]|\n",
      "|[4.0,5.0,6.0]|   [0.0,5.0,12.0]|\n",
      "+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Create some vector data; also works for sparse vectors\n",
    "data = [(Vectors.dense([1.0, 2.0, 3.0]),), (Vectors.dense([4.0, 5.0, 6.0]),)]\n",
    "df = spark.createDataFrame(data, [\"vector\"])\n",
    "transformer = ElementwiseProduct(scalingVec=Vectors.dense([0.0, 1.0, 2.0]),\n",
    "                                 inputCol=\"vector\", outputCol=\"transformedVector\")\n",
    "# Batch transform the vectors to create new column:\n",
    "transformer.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "exempt-standard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-----+\n",
      "|  a|  b|out_a|out_b|\n",
      "+---+---+-----+-----+\n",
      "|1.0|NaN|  1.0|  4.0|\n",
      "|2.0|NaN|  2.0|  4.0|\n",
      "|NaN|3.0|  3.0|  3.0|\n",
      "|4.0|4.0|  4.0|  4.0|\n",
      "|5.0|5.0|  5.0|  5.0|\n",
      "+---+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1.0, float(\"nan\")),\n",
    "    (2.0, float(\"nan\")),\n",
    "    (float(\"nan\"), 3.0),\n",
    "    (4.0, 4.0),\n",
    "    (5.0, 5.0)\n",
    "], [\"a\", \"b\"])\n",
    "\n",
    "imputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])\n",
    "model = imputer.fit(df)\n",
    "\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "laden-hopkins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+\n",
      "| id|            sentence|  sentiment|\n",
      "+---+--------------------+-----------+\n",
      "|  0|Hi I think pyspar...|      happy|\n",
      "|  1|All I want is a p...|indifferent|\n",
      "|  2|I finally underst...|  Fulfilled|\n",
      "|  3|Yet another sente...|indifferent|\n",
      "|  4|Why didn't I know...|        sad|\n",
      "|  5|          Yes, I can|      happy|\n",
      "+---+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_data_frame = spark.createDataFrame([\n",
    "    (0, \"Hi I think pyspark is cool \",\"happy\"),\n",
    "    (1, \"All I want is a pyspark cluster\",\"indifferent\"),\n",
    "    (2, \"I finally understand how ML works\",\"Fulfilled\"),\n",
    "    (3, \"Yet another sentence about pyspark and ML\",\"indifferent\"),\n",
    "    (4, \"Why didn't I know about mllib before\",\"sad\"),\n",
    "    (5, \"Yes, I can\",\"happy\")\n",
    "], [\"id\", \"sentence\", \"sentiment\"])\n",
    "\n",
    "sentence_data_frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-share",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------+-----------+-------------------------------------------------+\n",
      "|id |sentence                                 |sentiment  |words                                            |\n",
      "+---+-----------------------------------------+-----------+-------------------------------------------------+\n",
      "|0  |Hi I think pyspark is cool               |happy      |[hi, i, think, pyspark, is, cool]                |\n",
      "|1  |All I want is a pyspark cluster          |indifferent|[all, i, want, is, a, pyspark, cluster]          |\n",
      "|2  |I finally understand how ML works        |Fulfilled  |[i, finally, understand, how, ml, works]         |\n",
      "|3  |Yet another sentence about pyspark and ML|indifferent|[yet, another, sentence, about, pyspark, and, ml]|\n",
      "|4  |Why didn't I know about mllib before     |sad        |[why, didn't, i, know, about, mllib, before]     |\n",
      "|5  |Yes, I can                               |happy      |[yes,, i, can]                                   |\n",
      "+---+-----------------------------------------+-----------+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "tokenized = tokenizer.transform(sentence_data_frame)\n",
    "\n",
    "tokenized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "exact-trick",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/04 19:06:14 WARN StopWordsRemover: Default locale set was [en_GT]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+-------------------------------------+\n",
      "|words                                            |meaningful_words                     |\n",
      "+-------------------------------------------------+-------------------------------------+\n",
      "|[hi, i, think, pyspark, is, cool]                |[hi, think, pyspark, cool]           |\n",
      "|[all, i, want, is, a, pyspark, cluster]          |[want, pyspark, cluster]             |\n",
      "|[i, finally, understand, how, ml, works]         |[finally, understand, ml, works]     |\n",
      "|[yet, another, sentence, about, pyspark, and, ml]|[yet, another, sentence, pyspark, ml]|\n",
      "|[why, didn't, i, know, about, mllib, before]     |[know, mllib]                        |\n",
      "|[yes,, i, can]                                   |[yes,]                               |\n",
      "+-------------------------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"meaningful_words\")\n",
    "meaningful_data_frame = remover.transform(tokenized)\n",
    "\n",
    "meaningful_data_frame.select(\"words\",\"meaningful_words\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "apparent-cornwall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+--------------------+--------------------+-------------+\n",
      "| id|            sentence|  sentiment|               words|    meaningful_words|categoryIndex|\n",
      "+---+--------------------+-----------+--------------------+--------------------+-------------+\n",
      "|  0|Hi I think pyspar...|      happy|[hi, i, think, py...|[hi, think, pyspa...|          0.0|\n",
      "|  1|All I want is a p...|indifferent|[all, i, want, is...|[want, pyspark, c...|          1.0|\n",
      "|  2|I finally underst...|  Fulfilled|[i, finally, unde...|[finally, underst...|          2.0|\n",
      "|  3|Yet another sente...|indifferent|[yet, another, se...|[yet, another, se...|          1.0|\n",
      "|  4|Why didn't I know...|        sad|[why, didn't, i, ...|       [know, mllib]|          3.0|\n",
      "|  5|          Yes, I can|      happy|      [yes,, i, can]|              [yes,]|          0.0|\n",
      "+---+--------------------+-----------+--------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(meaningful_data_frame).transform(meaningful_data_frame)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "billion-mileage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, sentence: string, sentiment: string, words: array<string>, meaningful_words: array<string>, categoryIndex: double]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "decimal-grenada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sentence_id: bigint, happy: double, indifferent: double, Fulfilled: double, sad: double]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data_frame = spark.createDataFrame([\n",
    "    (0, 0.01,0.43,0.3,0.5),\n",
    "    (1, 0.097,0.21,0.2,0.9),\n",
    "    (2, 0.4,0.329,0.97,0.4),\n",
    "    (3, 0.7,0.4,0.3,0.87),\n",
    "    (4, 0.34,0.4,0.3,0.78),\n",
    "    (5, 0.1,0.3,0.31,0.29)\n",
    "], [\"sentence_id\", \"happy\", \"indifferent\",\"Fulfilled\",\"sad\"])\n",
    "\n",
    "sentiment_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "functioning-legislature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|happy|\n",
      "+-----+\n",
      "| 0.01|\n",
      "|0.097|\n",
      "|  0.4|\n",
      "|  0.7|\n",
      "| 0.34|\n",
      "|  0.1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "casted_data_frame = sentiment_data_frame.selectExpr(\"cast(happy as double)\")\n",
    "casted_data_frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "hundred-draft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentence_id: long (nullable = true)\n",
      " |-- happy: double (nullable = true)\n",
      " |-- indifferent: double (nullable = true)\n",
      " |-- Fulfilled: double (nullable = true)\n",
      " |-- sad: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_data_frame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "secure-cursor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(20,[6,8,13,16],[...|(20,[6,8,13,16],[...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|(20,[0,2,7,13,15,...|(20,[0,2,7,13,15,...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|(20,[3,4,6,11,19]...|(20,[3,4,6,11,19]...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "solar-copyright",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(20,[6,8,13,16],[...|(20,[6,8,13,16],[...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|(20,[0,2,7,13,15,...|(20,[0,2,7,13,15,...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|(20,[3,4,6,11,19]...|(20,[3,4,6,11,19]...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf])\n",
    "model = pipeline.fit(sentenceData)\n",
    "rescaledData = model.transform(sentenceData)\n",
    "rescaledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06303698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-with-spark-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
